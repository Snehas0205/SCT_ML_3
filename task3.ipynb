{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1efc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 4000/4000 [00:10<00:00, 395.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution - Cats (0): 1980, Dogs (1): 2020\n",
      "Data loaded successfully. Total samples: 4000\n",
      "Training samples: 3200\n",
      "Testing samples: 800\n",
      "\n",
      "Training the SVM classifier...\n",
      "Training completed in 9.06 seconds.\n",
      "\n",
      "Evaluating the model...\n",
      "Accuracy: 75.25%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Cat       0.74      0.77      0.75       396\n",
      "         Dog       0.76      0.74      0.75       404\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.75      0.75      0.75       800\n",
      "weighted avg       0.75      0.75      0.75       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --- 1. DATA PREPARATION ---\n",
    "\n",
    "def load_and_preprocess_data(data_dir, image_size=(64, 64), sample_size=4000):\n",
    "    \"\"\"\n",
    "    Loads images, preprocesses them, and extracts HOG features.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the training data directory.\n",
    "        image_size (tuple): The size to resize images to.\n",
    "        sample_size (int): The number of images to process for faster runs.\n",
    "                           Set to None to process all images.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get a list of image files and shuffle it\n",
    "    image_files = os.listdir(data_dir)\n",
    "    np.random.shuffle(image_files)\n",
    "    \n",
    "    # Limit the number of images to process if sample_size is set\n",
    "    if sample_size:\n",
    "        image_files = image_files[:sample_size]\n",
    "\n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "\n",
    "    # Use tqdm for a progress bar\n",
    "    for filename in tqdm(image_files, desc=\"Extracting Features\"):\n",
    "        try:\n",
    "            # Construct full path and read the image in grayscale\n",
    "            img_path = os.path.join(data_dir, filename)\n",
    "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            \n",
    "            # Skip if image is not loaded correctly\n",
    "            if image is None:\n",
    "                print(f\"Warning: Could not read image {filename}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Resize the image\n",
    "            resized_image = cv2.resize(image, image_size)\n",
    "\n",
    "            # --- Feature Extraction: Histogram of Oriented Gradients (HOG) ---\n",
    "            hog_features = hog(resized_image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                               cells_per_block=(2, 2), block_norm='L2-Hys', visualize=False)\n",
    "            \n",
    "            features.append(hog_features)\n",
    "\n",
    "            # --- Labeling: 'cat' -> 0, 'dog' -> 1 ---\n",
    "            if 'cat' in filename:\n",
    "                labels.append(0)\n",
    "            elif 'dog' in filename:\n",
    "                labels.append(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "\n",
    "    # Convert lists to numpy arrays for scikit-learn\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "\n",
    "# --- 2. MODEL TRAINING & EVALUATION ---\n",
    "\n",
    "# Define the path to your dataset\n",
    "DATASET_PATH = 'train'\n",
    "\n",
    "# Load data (using a sample of 4000 images for a reasonably quick run)\n",
    "# For a full run, set sample_size=None, but be aware it will be very slow.\n",
    "features, labels = load_and_preprocess_data(DATASET_PATH, sample_size=4000)\n",
    "\n",
    "# --- Diagnostic Check ---\n",
    "# Check the distribution of the loaded labels to ensure we have both classes.\n",
    "print(f\"\\nLabel distribution - Cats (0): {np.count_nonzero(labels == 0)}, Dogs (1): {np.count_nonzero(labels == 1)}\")\n",
    "\n",
    "if len(features) > 0 and np.count_nonzero(labels == 0) > 0 and np.count_nonzero(labels == 1) > 0:\n",
    "    print(f\"Data loaded successfully. Total samples: {len(features)}\")\n",
    "    \n",
    "    # Split data into training (80%) and testing (20%) sets\n",
    "    # stratify=labels ensures that the train/test split has a similar proportion of cats and dogs.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "\n",
    "    # Initialize the Support Vector Classifier\n",
    "    # kernel='rbf' is a powerful, non-linear kernel.\n",
    "    # C=10 is a regularization parameter.\n",
    "    print(\"\\nTraining the SVM classifier...\")\n",
    "    svm_model = SVC(kernel='rbf', C=10, random_state=42, gamma='scale')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    # --- 3. PREDICTION & PERFORMANCE ---\n",
    "    print(\"\\nEvaluating the model...\")\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Calculate and print accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Print a detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Cat', 'Dog']))\n",
    "\n",
    "else:\n",
    "    print(\"\\nError: Not enough data or classes to train the model. Please check your 'train' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc7940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
